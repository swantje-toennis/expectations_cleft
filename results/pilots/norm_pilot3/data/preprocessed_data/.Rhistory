library(betareg)
library(broom)
library(car)
library(plyr)
library(dplyr)
library(emmeans)
library(ez)
library(fitdistrplus)
library(gplots)
library(ggplot2)
library(ggpubr)
library(languageR)
library(lme4)
library(lmtest)
library(logspline)
library(MASS)
library(multcompView)
library(nlme)
library(ordinal)
library(psy811)
library(psych)
library(rcompanion)
library(readr)
library(readxl)
library(rstatix)
library(stringr)
library(tidyr)
library(tidyselect)
library(tidyverse)
library(readxl)
library(readr)
library(ggplot2)
library(ggpubr)
library(plyr)
library(dplyr)
library(ez)
library(lme4)
library(languageR)
library(car)
library(MASS)
library(fitdistrplus)
library(ordinal)
library(nlme)
library(logspline)
library(gplots)
library(stringr)
library(tidyr)
library(broom)
library(tidyselect)
library(tidyverse)
library(psy811)
library(mixedup)
#library(betareg)
#library(emmeans)
#library(lmtest)
#library(multcompView)
#library(psych)
#library(rcompanion)
#library(rstatix)
########################################################
setwd("C:/Arbeit/Expectedness/experiments/pretest/pilot3/results/preprocessed_data")
source("C:/Arbeit/R statistics/mer-utils.r")
source("C:/Arbeit/R statistics/regression-utils.r")
source("C:/Arbeit/R statistics/diagnostic_fcns.r")
source("C:/Arbeit/R statistics/boot_glmm.r")
source("C:/Arbeit/R statistics/helpers.r")
#### read preprocessed data ###########
data <- read.csv("C:/Arbeit/Expectedness/experiments/pretest/pilot3/results/preprocessed_data/data.csv", sep = ";")
#### Interpret expectedness judgment as numbers and context, question and list as a factors #####
data$expec<-as.numeric(data$expec)
data$cond_c<-as.factor(data$cond_c)
data$cond_q<-as.factor(data$cond_q)
data$list <- as.factor(data$list)
str(data)
##### means and sd for irrelevant q ##########
data_irr %>% group_by(cond_c) %>%
summarize(M = mean(expec), SD = sd(expec))
##### means and sd for then-q ##########
data_then %>% group_by(cond_c) %>%
summarize(M = mean(expec), SD = sd(expec))
##### Take out all questions except pq1 ##########
data_pq1 <- subset(data, cond_q=="pq1")
str(data_pq1)
##### Take out all questions except pq3 ##########
data_pq3 <- subset(data, cond_q=="pq3")
#######################
####   Models #########
######################
#### model 1 -- Without random effects
model1 <- lm(expec ~ cond_c, data = data_pq1)
### Sanity check of model 1 #####
summary(model1)
#### get the p-values for model1 ######
get_pvalues(model1)
#### model 2 -- With varying intercepts and slopes
model2 <- lmer(expec ~ cond_c + (1 + cond_c|id) + (1 + cond_c|target_no), data = data_pq1)
### Sanity check of model 2 ######
kappa.mer(model2)   ## good!
max(vif.mer(model2)) ## good (checks collinearity)
overdisp.test(model2) ## not good! Is it because the sd is very high compared to the mean?
diagnostics.plot(model2) ##
lev.thresh(model2) ## good.
summary(model2)
#### get the p-values for model2 ######
get_pvalues(model2)
#### model 3 without the (only) predictor context
model3 <- lmer(expec ~ 1 + (1 + cond_c|id) + (1 + cond_c|target_no), data = data_pq1)
summary(model3)
#### Problem? ###
isSingular(model3, tol = 1e-4)
##### compare the two models with and without predictor######
anova(model2, model3)
#### model 4 -- With varying intercepts
model4 <- lmer(expec ~ cond_c + (1|id) + (1|target_no), data = data_pq1)
summary(model4)
get_pvalues(model4)
#### model 5 without the (only) predictor context
model5 <- lmer(expec ~ 1 + (1|id) + (1|target_no), data = data_pq1)
summary(model5)
##### compare the two models with and without predictor (both including varying intercepts)######
anova(model4, model5)
###############################################
#     Analysis with normalized data
###############################################
###### normalize data ########
zdata1 <- data %>% group_by(id,target_no,cond_c)%>%
summarize(id, target_no, cond_c, norm_exp = 100*expec/sum(expec), expec, cond_q, list)
normalized_data <- zdata1 %>%
mutate_at("norm_exp", str_replace, "NaN", "0")
normalized_data$norm_exp <- as.numeric(normalized_data$norm_exp)
str(normalized_data)
##### Take out all questions except pq1 ##########
normalized_data_pq1 <- subset(normalized_data, cond_q=="pq1")
normalized_data_pq1$norm_exp<-as.numeric(normalized_data_pq1$norm_exp)
str(normalized_data_pq1)
##### show the mean and standard deviation for the two conditions ##########
normalized_data_pq1 %>% group_by(cond_c) %>%
summarize(M = mean(norm_exp), SD = sd(norm_exp))
#######################
####   Models #########
#######################
##### t-test #######
test<-t.test(norm_exp ~ cond_c, data=normalized_data_pq1, var.equal=FALSE, na.rm=TRUE)
print(test)
#### model 1 -- Without random effects
model1 <- lm(norm_exp ~ cond_c, data = normalized_data_pq1)
### Sanity check of model 1 ######
#???
summary(model1)
#head(fitted(model1))  #Shows the fitted values: It should be just two values repeating
#### get the p-values for model1 ######
get_pvalues(model1)
#### model 2 -- With varying intercepts and slopes for subject and item
model2 <- lmer(norm_exp ~ cond_c + (1 + cond_c|id) + (1 + cond_c|target_no), data = normalized_data_pq1)
### Sanity check of model 2 ######
#<10 is reasonable collinearity, <30 is moderate collinearity, over 30 is troubling
kappa.mer(model2)   ## good!
#vif over 5 is troubling, over 2.5 is moderate (checks collinearity)
max(vif.mer(model2)) ## good (checks collinearity)
# dispersion parameter should not be much larger than 1
overdisp.test(model2) ## not good!
# residuals should be normally distributed, qq-plot plot should not show strong deviations from the line,
# residuals against fitted values should show a rather random pattern
diagnostics.plot(model2) ## good!
#lev.thresh should be close to 0, means that there are no influential cases
lev.thresh(model2) ## good.
#### Summary model 2  ######
summary(model2)
random_effects_model2 <- extract_random_effects(model2)
#### get the p-values for model2 ######
get_pvalues(model2)
#### model 4 -- With only varying intercepts for item and varying intercepts and slopes for item
model4 <- lmer(norm_exp ~ cond_c + (1+ cond_c|id) + (1|target_no), data = normalized_data_pq1)
summary(model4)
#### compare model 2 and 4
anova(model2,model4)
